{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Comparison & Selection Notebook\n",
    "## Comprehensive Analysis for Production-Grade NLP\n",
    "**Objective**: Compare multiple transformer models for token classification (NER) and select the best model based on performance metrics, speed, and size.\n",
    "**Key Features**:\n",
    "- Robust comparison of 4 multilingual transformer models\n",
    "- Advanced error handling and recovery mechanisms\n",
    "- Comprehensive metrics tracking (accuracy, speed, size)\n",
    "- Production deployment recommendations\n",
    "- Version compatibility checks\n",
    "**What a Top 0.1% Practitioner Would Consider**:\n",
    "1. **Precision in Evaluation**: Beyond simple F1 scores, examining per-entity performance\n",
    "2. **Resource Efficiency**: Careful measurement of memory footprint and inference speed\n",
    "3. **Training Stability**: Monitoring for NaN losses and convergence patterns\n",
    "4. **Production Readiness**: Considering model size and quantization potential\n",
    "5. **Error Analysis**: Detailed examination of failure cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration Setup\n",
    "**Top Practitioner Insights**:\n",
    "- Version compatibility is crucial for reproducibility\n",
    "- Label schema should be carefully validated\n",
    "- Batch sizes should be tuned for each model's memory requirements\n",
    "- Warmup ratio helps with training stability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForTokenClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForTokenClassification\n",
    ")\n",
    "from datasets import load_dataset, DatasetDict, Dataset\n",
    "from seqeval.metrics import classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Configure logging\n",
    "import logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\",\n",
    "    handlers=[\n",
    "        logging.FileHandler(\"model_comparison.log\"),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration Class\n",
    "**Top Practitioner Touches**:\n",
    "- Explicit version checking for compatibility\n",
    "- Detailed model metadata for informed selection\n",
    "- Memory-efficient default parameters\n",
    "- Comprehensive validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelComparisonConfig:\n",
    "    def __init__(self):\n",
    "        # Core label mapping\n",
    "        self.label_list = [\"O\", \"B-PRODUCT\", 'B-LOC', \"I-PRODUCT\", \"B-PRICE\", \"I-PRICE\", \"B-PHONE\", \"I-PHONE\"]\n",
    "        self.label2id = {label: i for i, label in enumerate(self.label_list)}\n",
    "        self.id2label = {i: label for i, label in enumerate(self.label_list)}\n",
    "        \n",
    "        # Model candidates to compare\n",
    "        self.model_candidates = {\n",
    "            \"xlm-roberta-base\": {\n",
    "                \"name\": \"xlm-roberta-base\",\n",
    "                \"description\": \"Large multilingual model optimized for cross-lingual tasks\",\n",
    "                \"expected_perf\": \"High accuracy, moderate speed\",\n",
    "                \"size\": \"~2.5GB\"\n",
    "            },\n",
    "            \"distilbert-base-multilingual-cased\": {\n",
    "                \"name\": \"distilbert-base-multilingual-cased\",\n",
    "                \"description\": \"Distilled version of multilingual BERT, faster inference\",\n",
    "                \"expected_perf\": \"Good accuracy, fast speed\",\n",
    "                \"size\": \"~500MB\"\n",
    "            },\n",
    "            \"bert-base-multilingual-cased\": {\n",
    "                \"name\": \"bert-base-multilingual-cased\",\n",
    "                \"description\": \"Original multilingual BERT model\",\n",
    "                \"expected_perf\": \"Good accuracy, moderate speed\",\n",
    "                \"size\": \"~1.5GB\"\n",
    "            },\n",
    "            \"google/rembert\": {\n",
    "                \"name\": \"google/rembert\",\n",
    "                \"description\": \"Rethinking embedding and transformer model, handles long sequences well\",\n",
    "                \"expected_perf\": \"High accuracy, slower speed\",\n",
    "                \"size\": \"~5GB\"\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Training config\n",
    "        self.max_seq_length = 128\n",
    "        self.batch_size = 16\n",
    "        self.num_train_epochs = 3\n",
    "        self.learning_rate = 2e-5\n",
    "        self.weight_decay = 0.01\n",
    "        self.warmup_ratio = 0.1\n",
    "        \n",
    "        # Paths\n",
    "        self.data_dir = Path(\"data/labeled\")\n",
    "        self.output_dir = Path(\"models/comparison\")\n",
    "        self.results_file = self.output_dir / \"comparison_results.json\"\n",
    "        \n",
    "        # Validation\n",
    "        self.validate()\n",
    "    \n",
    "    def validate(self):\n",
    "        \"\"\"Validate configuration\"\"\"\n",
    "        if not self.data_dir.exists():\n",
    "            raise FileNotFoundError(f\"Data directory not found: {self.data_dir}\")\n",
    "        self.output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "config = ModelComparisonConfig()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Preparation\n",
    "**Top Practitioner Considerations**:\n",
    "- Proper handling of token-label alignment\n",
    "- Validation of label distribution\n",
    "- Careful conll file parsing with error recovery\n",
    "- Dataset statistics collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-27 21:56:20,191 - __main__ - INFO - Loading and preparing dataset...\n",
      "2025-06-27 21:56:21,509 - __main__ - INFO - Loaded 2507 train and 627 val examples\n"
     ]
    }
   ],
   "source": [
    "def load_and_prepare_data():\n",
    "    \"\"\"Load and prepare the dataset for all models\"\"\"\n",
    "    logger.info(\"Loading and preparing dataset...\")\n",
    "    \n",
    "    # Load CONLL files\n",
    "    def parse_conll_file(file_path):\n",
    "        examples = []\n",
    "        current_example = {\"tokens\": [], \"tags\": []}\n",
    "        \n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if not line:  # Sentence boundary\n",
    "                    if current_example[\"tokens\"]:\n",
    "                        examples.append(current_example)\n",
    "                        current_example = {\"tokens\": [], \"tags\": []}\n",
    "                else:\n",
    "                    parts = line.split('\\t')\n",
    "                    if len(parts) == 2:\n",
    "                        token, tag = parts\n",
    "                        current_example[\"tokens\"].append(token)\n",
    "                        current_example[\"tags\"].append(tag if tag in config.label2id else \"O\")\n",
    "            \n",
    "            # Add last example if file doesn't end with newline\n",
    "            if current_example[\"tokens\"]:\n",
    "                examples.append(current_example)\n",
    "        \n",
    "        return examples\n",
    "    \n",
    "    train_data = parse_conll_file(config.data_dir / 'train.conll')\n",
    "    val_data = parse_conll_file(config.data_dir / 'val.conll')\n",
    "    \n",
    "    # Create dataset\n",
    "    dataset = DatasetDict({\n",
    "        'train': Dataset.from_list(train_data),\n",
    "        'val': Dataset.from_list(val_data)\n",
    "    })\n",
    "    \n",
    "    # Log statistics\n",
    "    logger.info(f\"Loaded {len(dataset['train'])} train and {len(dataset['val'])} val examples\")\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "dataset = load_and_prepare_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Core Training Components\n",
    "**Elite Practitioner Enhancements**:\n",
    "- Advanced label alignment handling\n",
    "- Comprehensive metrics calculation\n",
    "- Version-compatible training arguments\n",
    "- Memory-efficient data collation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples, tokenizer):\n",
    "    \"\"\"Tokenize text and align labels with tokens\"\"\"\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"tokens\"],\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        max_length=config.max_seq_length,\n",
    "        is_split_into_words=True,\n",
    "        return_offsets_mapping=False\n",
    "    )\n",
    "    \n",
    "    labels = []\n",
    "    for i, tags in enumerate(examples[\"tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        \n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:\n",
    "                try:\n",
    "                    label_ids.append(config.label2id[tags[word_idx]])\n",
    "                except IndexError:\n",
    "                    label_ids.append(-100)\n",
    "            else:\n",
    "                # Handle subword tokens\n",
    "                previous_tag = tags[previous_word_idx]\n",
    "                if previous_tag.startswith(\"B-\"):\n",
    "                    new_tag = \"I-\" + previous_tag[2:]\n",
    "                    label_ids.append(config.label2id.get(new_tag, -100))\n",
    "                elif previous_tag.startswith(\"I-\"):\n",
    "                    label_ids.append(config.label2id[tags[previous_word_idx]])\n",
    "                else:\n",
    "                    label_ids.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "        \n",
    "        labels.append(label_ids)\n",
    "    \n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "def compute_metrics(p):\n",
    "    \"\"\"Compute evaluation metrics using seqeval\"\"\"\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "    \n",
    "    # Remove ignored index (special tokens)\n",
    "    true_predictions = [\n",
    "        [config.id2label[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [config.id2label[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    \n",
    "    results = classification_report(true_labels, true_predictions, output_dict=True)\n",
    "    return {\n",
    "        \"precision\": results[\"weighted avg\"][\"precision\"],\n",
    "        \"recall\": results[\"weighted avg\"][\"recall\"],\n",
    "        \"f1\": results[\"weighted avg\"][\"f1-score\"],\n",
    "        \"accuracy\": results.get(\"accuracy\", 0)\n",
    "    }\n",
    "\n",
    "def train_and_evaluate_model(model_name):\n",
    "    \"\"\"Train and evaluate a single model\"\"\"\n",
    "    try:\n",
    "        model_output_dir = config.output_dir / model_name.replace(\"/\", \"_\")\n",
    "        model_output_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        logger.info(f\"\\n{'='*50}\")\n",
    "        logger.info(f\"Training and evaluating: {model_name}\")\n",
    "        logger.info(f\"{'='*50}\")\n",
    "        \n",
    "        # Load tokenizer and model\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        model = AutoModelForTokenClassification.from_pretrained(\n",
    "            model_name,\n",
    "            num_labels=len(config.label_list),\n",
    "            id2label=config.id2label,\n",
    "            label2id=config.label2id\n",
    "        )\n",
    "        \n",
    "        # Tokenize dataset\n",
    "        tokenized_dataset = dataset.map(\n",
    "            lambda x: tokenize_and_align_labels(x, tokenizer),\n",
    "            batched=True,\n",
    "            batch_size=32,\n",
    "            remove_columns=dataset[\"train\"].column_names\n",
    "        )\n",
    "        \n",
    "        # Training arguments\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=str(model_output_dir),\n",
    "            evaluation_strategy=\"epoch\",\n",
    "            save_strategy=\"epoch\",\n",
    "            learning_rate=config.learning_rate,\n",
    "            per_device_train_batch_size=config.batch_size,\n",
    "            per_device_eval_batch_size=config.batch_size,\n",
    "            num_train_epochs=config.num_train_epochs,\n",
    "            weight_decay=config.weight_decay,\n",
    "            load_best_model_at_end=True,\n",
    "            metric_for_best_model=\"f1\",\n",
    "            greater_is_better=True,\n",
    "            save_total_limit=2,\n",
    "            fp16=torch.cuda.is_available(),\n",
    "            warmup_ratio=config.warmup_ratio,\n",
    "            gradient_accumulation_steps=2,\n",
    "            logging_dir=str(model_output_dir / \"logs\"),\n",
    "            logging_steps=50,\n",
    "            report_to=\"none\"\n",
    "        )\n",
    "        \n",
    "        # Initialize Trainer\n",
    "        data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "        \n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=tokenized_dataset[\"train\"],\n",
    "            eval_dataset=tokenized_dataset[\"val\"],\n",
    "            data_collator=data_collator,\n",
    "            compute_metrics=compute_metrics,\n",
    "            tokenizer=tokenizer\n",
    "        )\n",
    "        \n",
    "        # Train and time it\n",
    "        start_time = time.time()\n",
    "        train_results = trainer.train()\n",
    "        training_time = time.time() - start_time\n",
    "        \n",
    "        # Evaluate\n",
    "        eval_results = trainer.evaluate()\n",
    "        \n",
    "        # Save model\n",
    "        trainer.save_model(str(model_output_dir))\n",
    "        tokenizer.save_pretrained(str(model_output_dir))\n",
    "        \n",
    "        # Measure inference speed\n",
    "        inference_speed = measure_inference_speed(model, tokenizer)\n",
    "        \n",
    "        # Calculate model size\n",
    "        model_size = calculate_model_size(model_output_dir)\n",
    "        \n",
    "        return {\n",
    "            \"model_name\": model_name,\n",
    "            \"training_time\": training_time,\n",
    "            \"inference_speed\": inference_speed,\n",
    "            \"model_size\": model_size,\n",
    "            \"metrics\": eval_results,\n",
    "            \"output_dir\": str(model_output_dir)\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error with model {model_name}: {str(e)}\")\n",
    "        return {\n",
    "            \"model_name\": model_name,\n",
    "            \"error\": str(e)\n",
    "        }\n",
    "\n",
    "def measure_inference_speed(model, tokenizer, num_samples=100):\n",
    "    \"\"\"Measure average inference time per sample\"\"\"\n",
    "    device = model.device\n",
    "    sample_texts = [\" \".join(ex[\"tokens\"]) for ex in dataset[\"val\"][:num_samples]]\n",
    "    \n",
    "    # Warmup\n",
    "    for text in sample_texts[:5]:\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=config.max_seq_length).to(device)\n",
    "        with torch.no_grad():\n",
    "            _ = model(**inputs)\n",
    "    \n",
    "    # Measure\n",
    "    start_time = time.time()\n",
    "    for text in sample_texts:\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=config.max_seq_length).to(device)\n",
    "        with torch.no_grad():\n",
    "            _ = model(**inputs)\n",
    "    \n",
    "    total_time = time.time() - start_time\n",
    "    return total_time / num_samples  # seconds per sample\n",
    "\n",
    "def calculate_model_size(model_dir):\n",
    "    \"\"\"Calculate model size in MB\"\"\"\n",
    "    total_size = 0\n",
    "    for path in Path(model_dir).rglob('*'):\n",
    "        if path.is_file():\n",
    "            total_size += path.stat().st_size\n",
    "    return total_size / (1024 * 1024)  # Convert to MB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-27 21:56:21,855 - __main__ - INFO - \n",
      "==================================================\n",
      "2025-06-27 21:56:21,855 - __main__ - INFO - Starting evaluation for: xlm-roberta-base\n",
      "2025-06-27 21:56:21,855 - __main__ - INFO - Description: Large multilingual model optimized for cross-lingual tasks\n",
      "2025-06-27 21:56:21,855 - __main__ - INFO - Expected: High accuracy, moderate speed\n",
      "2025-06-27 21:56:21,855 - __main__ - INFO - Size: ~2.5GB\n",
      "2025-06-27 21:56:21,870 - __main__ - INFO - \n",
      "==================================================\n",
      "2025-06-27 21:56:21,870 - __main__ - INFO - Training and evaluating: xlm-roberta-base\n",
      "2025-06-27 21:56:21,870 - __main__ - INFO - ==================================================\n",
      "Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b160ea0189b43318e0d566c9291ce27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2507 [00:06<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa05d4167f514379a9db0f6e9c6b84cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/627 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-27 21:57:34,700 - __main__ - ERROR - Error with model xlm-roberta-base: __init__() got an unexpected keyword argument 'evaluation_strategy'\n",
      "2025-06-27 21:57:58,103 - __main__ - INFO - Completed evaluation for: xlm-roberta-base\n",
      "2025-06-27 21:57:58,142 - __main__ - INFO - \n",
      "==================================================\n",
      "2025-06-27 21:57:58,147 - __main__ - INFO - Starting evaluation for: distilbert-base-multilingual-cased\n",
      "2025-06-27 21:57:58,151 - __main__ - INFO - Description: Distilled version of multilingual BERT, faster inference\n",
      "2025-06-27 21:57:58,155 - __main__ - INFO - Expected: Good accuracy, fast speed\n",
      "2025-06-27 21:57:58,157 - __main__ - INFO - Size: ~500MB\n",
      "2025-06-27 21:57:58,171 - __main__ - INFO - \n",
      "==================================================\n",
      "2025-06-27 21:57:58,174 - __main__ - INFO - Training and evaluating: distilbert-base-multilingual-cased\n",
      "2025-06-27 21:57:58,177 - __main__ - INFO - ==================================================\n",
      "Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5815128549140b18f1862b4e436792e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2507 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9216a62e9f3f4d23b3eb434bf24f9a15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/627 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-27 21:58:21,964 - __main__ - ERROR - Error with model distilbert-base-multilingual-cased: __init__() got an unexpected keyword argument 'evaluation_strategy'\n",
      "2025-06-27 21:58:22,073 - __main__ - INFO - Completed evaluation for: distilbert-base-multilingual-cased\n",
      "2025-06-27 21:58:22,073 - __main__ - INFO - \n",
      "==================================================\n",
      "2025-06-27 21:58:22,089 - __main__ - INFO - Starting evaluation for: bert-base-multilingual-cased\n",
      "2025-06-27 21:58:22,089 - __main__ - INFO - Description: Original multilingual BERT model\n",
      "2025-06-27 21:58:22,089 - __main__ - INFO - Expected: Good accuracy, moderate speed\n",
      "2025-06-27 21:58:22,103 - __main__ - INFO - Size: ~1.5GB\n",
      "2025-06-27 21:58:22,105 - __main__ - INFO - \n",
      "==================================================\n",
      "2025-06-27 21:58:22,105 - __main__ - INFO - Training and evaluating: bert-base-multilingual-cased\n",
      "2025-06-27 21:58:22,105 - __main__ - INFO - ==================================================\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efbb3fc575594a6d8662e323aa6d4343",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2507 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "782a20f78f624855b05de92940f18819",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/627 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-27 21:58:51,226 - __main__ - ERROR - Error with model bert-base-multilingual-cased: __init__() got an unexpected keyword argument 'evaluation_strategy'\n",
      "2025-06-27 21:58:51,367 - __main__ - INFO - Completed evaluation for: bert-base-multilingual-cased\n",
      "2025-06-27 21:58:51,367 - __main__ - INFO - \n",
      "==================================================\n",
      "2025-06-27 21:58:51,367 - __main__ - INFO - Starting evaluation for: google/rembert\n",
      "2025-06-27 21:58:51,383 - __main__ - INFO - Description: Rethinking embedding and transformer model, handles long sequences well\n",
      "2025-06-27 21:58:51,383 - __main__ - INFO - Expected: High accuracy, slower speed\n",
      "2025-06-27 21:58:51,383 - __main__ - INFO - Size: ~5GB\n",
      "2025-06-27 21:58:51,383 - __main__ - INFO - \n",
      "==================================================\n",
      "2025-06-27 21:58:51,383 - __main__ - INFO - Training and evaluating: google/rembert\n",
      "2025-06-27 21:58:51,399 - __main__ - INFO - ==================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de9f20bc631c48fcb2c0185c7ac80d9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:  46%|####5     | 1.94G/4.24G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "461e376e50b74af2b8ad4842a9e74aed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.30G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RemBertForTokenClassification were not initialized from the model checkpoint at google/rembert and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa127299ba424f66aaa9615eba837834",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2507 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a739eb940d5d4a49ad8903dd0be3eb78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/627 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-27 22:15:30,194 - __main__ - ERROR - Error with model google/rembert: __init__() got an unexpected keyword argument 'evaluation_strategy'\n",
      "2025-06-27 22:15:32,630 - __main__ - INFO - Completed evaluation for: google/rembert\n"
     ]
    }
   ],
   "source": [
    "def run_model_comparison():\n",
    "    \"\"\"Run comparison of all model candidates\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for model_name, model_info in config.model_candidates.items():\n",
    "        logger.info(f\"\\n{'='*50}\")\n",
    "        logger.info(f\"Starting evaluation for: {model_name}\")\n",
    "        logger.info(f\"Description: {model_info['description']}\")\n",
    "        logger.info(f\"Expected: {model_info['expected_perf']}\")\n",
    "        logger.info(f\"Size: {model_info['size']}\")\n",
    "        \n",
    "        result = train_and_evaluate_model(model_name)\n",
    "        results.append(result)\n",
    "        \n",
    "        # Save intermediate results\n",
    "        with open(config.results_file, 'w') as f:\n",
    "            json.dump(results, f, indent=2)\n",
    "        \n",
    "        logger.info(f\"Completed evaluation for: {model_name}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run the comparison\n",
    "comparison_results = run_model_comparison()\n",
    "\n",
    "# Save final results\n",
    "with open(config.results_file, 'w') as f:\n",
    "    json.dump(comparison_results, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Performance Measurement\n",
    "**Top 0.1% Practitioner Metrics**:\n",
    "- Precise inference timing\n",
    "- Memory footprint analysis\n",
    "- Robust evaluation fallbacks\n",
    "- Comprehensive benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Could not interpret value `Model` for `x`. An entry with this name does not appear in `data`.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 76\u001b[0m\n\u001b[0;32m     73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m df, best_model\n\u001b[0;32m     75\u001b[0m \u001b[38;5;66;03m# Analyze and visualize results\u001b[39;00m\n\u001b[1;32m---> 76\u001b[0m results_df, best_model \u001b[38;5;241m=\u001b[39m \u001b[43manalyze_results\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcomparison_results\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[6], line 26\u001b[0m, in \u001b[0;36manalyze_results\u001b[1;34m(results)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# F1-Score comparison\u001b[39;00m\n\u001b[0;32m     25\u001b[0m plt\u001b[38;5;241m.\u001b[39msubplot(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 26\u001b[0m \u001b[43msns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbarplot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mModel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mF1-Score\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m plt\u001b[38;5;241m.\u001b[39mtitle(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mF1-Score Comparison\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     28\u001b[0m plt\u001b[38;5;241m.\u001b[39mxticks(rotation\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m45\u001b[39m)\n",
      "File \u001b[1;32m~\\Desktop\\EthioMart-Amharic-NER\\venv\\lib\\site-packages\\seaborn\\categorical.py:2341\u001b[0m, in \u001b[0;36mbarplot\u001b[1;34m(data, x, y, hue, order, hue_order, estimator, errorbar, n_boot, seed, units, weights, orient, color, palette, saturation, fill, hue_norm, width, dodge, gap, log_scale, native_scale, formatter, legend, capsize, err_kws, ci, errcolor, errwidth, ax, **kwargs)\u001b[0m\n\u001b[0;32m   2338\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m estimator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mlen\u001b[39m:\n\u001b[0;32m   2339\u001b[0m     estimator \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msize\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 2341\u001b[0m p \u001b[38;5;241m=\u001b[39m \u001b[43m_CategoricalAggPlotter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2342\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2343\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvariables\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweights\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2344\u001b[0m \u001b[43m    \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2345\u001b[0m \u001b[43m    \u001b[49m\u001b[43morient\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2346\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2347\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlegend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlegend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2348\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2350\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ax \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   2351\u001b[0m     ax \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39mgca()\n",
      "File \u001b[1;32m~\\Desktop\\EthioMart-Amharic-NER\\venv\\lib\\site-packages\\seaborn\\categorical.py:67\u001b[0m, in \u001b[0;36m_CategoricalPlotter.__init__\u001b[1;34m(self, data, variables, order, orient, require_numeric, color, legend)\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m     57\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m     58\u001b[0m     data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     64\u001b[0m     legend\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     65\u001b[0m ):\n\u001b[1;32m---> 67\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvariables\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvariables\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# This method takes care of some bookkeeping that is necessary because the\u001b[39;00m\n\u001b[0;32m     70\u001b[0m     \u001b[38;5;66;03m# original categorical plots (prior to the 2021 refactor) had some rules that\u001b[39;00m\n\u001b[0;32m     71\u001b[0m     \u001b[38;5;66;03m# don't fit exactly into VectorPlotter logic. It may be wise to have a second\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     76\u001b[0m     \u001b[38;5;66;03m# default VectorPlotter rules. If we do decide to make orient part of the\u001b[39;00m\n\u001b[0;32m     77\u001b[0m     \u001b[38;5;66;03m# _base variable assignment, we'll want to figure out how to express that.\u001b[39;00m\n\u001b[0;32m     78\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_format \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwide\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m orient \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mh\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[1;32m~\\Desktop\\EthioMart-Amharic-NER\\venv\\lib\\site-packages\\seaborn\\_base.py:634\u001b[0m, in \u001b[0;36mVectorPlotter.__init__\u001b[1;34m(self, data, variables)\u001b[0m\n\u001b[0;32m    629\u001b[0m \u001b[38;5;66;03m# var_ordered is relevant only for categorical axis variables, and may\u001b[39;00m\n\u001b[0;32m    630\u001b[0m \u001b[38;5;66;03m# be better handled by an internal axis information object that tracks\u001b[39;00m\n\u001b[0;32m    631\u001b[0m \u001b[38;5;66;03m# such information and is set up by the scale_* methods. The analogous\u001b[39;00m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;66;03m# information for numeric axes would be information about log scales.\u001b[39;00m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_var_ordered \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m}  \u001b[38;5;66;03m# alt., used DefaultDict\u001b[39;00m\n\u001b[1;32m--> 634\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43massign_variables\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvariables\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    636\u001b[0m \u001b[38;5;66;03m# TODO Lots of tests assume that these are called to initialize the\u001b[39;00m\n\u001b[0;32m    637\u001b[0m \u001b[38;5;66;03m# mappings to default values on class initialization. I'd prefer to\u001b[39;00m\n\u001b[0;32m    638\u001b[0m \u001b[38;5;66;03m# move away from that and only have a mapping when explicitly called.\u001b[39;00m\n\u001b[0;32m    639\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m var \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhue\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msize\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstyle\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[1;32m~\\Desktop\\EthioMart-Amharic-NER\\venv\\lib\\site-packages\\seaborn\\_base.py:679\u001b[0m, in \u001b[0;36mVectorPlotter.assign_variables\u001b[1;34m(self, data, variables)\u001b[0m\n\u001b[0;32m    674\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    675\u001b[0m     \u001b[38;5;66;03m# When dealing with long-form input, use the newer PlotData\u001b[39;00m\n\u001b[0;32m    676\u001b[0m     \u001b[38;5;66;03m# object (internal but introduced for the objects interface)\u001b[39;00m\n\u001b[0;32m    677\u001b[0m     \u001b[38;5;66;03m# to centralize / standardize data consumption logic.\u001b[39;00m\n\u001b[0;32m    678\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_format \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlong\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 679\u001b[0m     plot_data \u001b[38;5;241m=\u001b[39m \u001b[43mPlotData\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvariables\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    680\u001b[0m     frame \u001b[38;5;241m=\u001b[39m plot_data\u001b[38;5;241m.\u001b[39mframe\n\u001b[0;32m    681\u001b[0m     names \u001b[38;5;241m=\u001b[39m plot_data\u001b[38;5;241m.\u001b[39mnames\n",
      "File \u001b[1;32m~\\Desktop\\EthioMart-Amharic-NER\\venv\\lib\\site-packages\\seaborn\\_core\\data.py:58\u001b[0m, in \u001b[0;36mPlotData.__init__\u001b[1;34m(self, data, variables)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m     52\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m     53\u001b[0m     data: DataSource,\n\u001b[0;32m     54\u001b[0m     variables: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, VariableSpec],\n\u001b[0;32m     55\u001b[0m ):\n\u001b[0;32m     57\u001b[0m     data \u001b[38;5;241m=\u001b[39m handle_data_source(data)\n\u001b[1;32m---> 58\u001b[0m     frame, names, ids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_assign_variables\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvariables\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframe \u001b[38;5;241m=\u001b[39m frame\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnames \u001b[38;5;241m=\u001b[39m names\n",
      "File \u001b[1;32m~\\Desktop\\EthioMart-Amharic-NER\\venv\\lib\\site-packages\\seaborn\\_core\\data.py:232\u001b[0m, in \u001b[0;36mPlotData._assign_variables\u001b[1;34m(self, data, variables)\u001b[0m\n\u001b[0;32m    230\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    231\u001b[0m         err \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn entry with this name does not appear in `data`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 232\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(err)\n\u001b[0;32m    234\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    235\u001b[0m \n\u001b[0;32m    236\u001b[0m     \u001b[38;5;66;03m# Otherwise, assume the value somehow represents data\u001b[39;00m\n\u001b[0;32m    237\u001b[0m \n\u001b[0;32m    238\u001b[0m     \u001b[38;5;66;03m# Ignore empty data structures\u001b[39;00m\n\u001b[0;32m    239\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(val, Sized) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(val) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[1;31mValueError\u001b[0m: Could not interpret value `Model` for `x`. An entry with this name does not appear in `data`."
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def analyze_results(results):\n",
    "    \"\"\"Analyze and visualize comparison results\"\"\"\n",
    "    # Filter out failed models\n",
    "    successful_results = [r for r in results if \"error\" not in r]\n",
    "    \n",
    "    # Create DataFrame for analysis\n",
    "    df_data = []\n",
    "    for result in successful_results:\n",
    "        df_data.append({\n",
    "            \"Model\": result[\"model_name\"],\n",
    "            \"F1-Score\": result[\"metrics\"][\"eval_f1\"],\n",
    "            \"Precision\": result[\"metrics\"][\"eval_precision\"],\n",
    "            \"Recall\": result[\"metrics\"][\"eval_recall\"],\n",
    "            \"Training Time (min)\": result[\"training_time\"] / 60,\n",
    "            \"Inference Speed (ms)\": result[\"inference_speed\"] * 1000,\n",
    "            \"Model Size (MB)\": result[\"model_size\"]\n",
    "        })\n",
    "    \n",
    "    df = pd.DataFrame(df_data)\n",
    "    \n",
    "    # Plot comparison metrics\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    # F1-Score comparison\n",
    "    plt.subplot(2, 2, 1)\n",
    "    sns.barplot(data=df, x=\"Model\", y=\"F1-Score\")\n",
    "    plt.title(\"F1-Score Comparison\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.ylim(0, 1)\n",
    "    \n",
    "    # Training Time comparison\n",
    "    plt.subplot(2, 2, 2)\n",
    "    sns.barplot(data=df, x=\"Model\", y=\"Training Time (min)\")\n",
    "    plt.title(\"Training Time Comparison (minutes)\")\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    # Inference Speed comparison\n",
    "    plt.subplot(2, 2, 3)\n",
    "    sns.barplot(data=df, x=\"Model\", y=\"Inference Speed (ms)\")\n",
    "    plt.title(\"Inference Speed Comparison (milliseconds)\")\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    # Model Size comparison\n",
    "    plt.subplot(2, 2, 4)\n",
    "    sns.barplot(data=df, x=\"Model\", y=\"Model Size (MB)\")\n",
    "    plt.title(\"Model Size Comparison (MB)\")\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(config.output_dir / \"model_comparison.png\")\n",
    "    plt.show()\n",
    "    \n",
    "    # Print comprehensive comparison\n",
    "    print(\"\\nModel Comparison Summary:\")\n",
    "    print(df.to_markdown(index=False))\n",
    "    \n",
    "    # Select best model based on weighted criteria\n",
    "    df[\"Weighted Score\"] = (\n",
    "        0.5 * df[\"F1-Score\"] + \n",
    "        0.2 * (1 - (df[\"Inference Speed (ms)\"] / df[\"Inference Speed (ms)\"].max())) +\n",
    "        0.2 * (1 - (df[\"Model Size (MB)\"] / df[\"Model Size (MB)\"].max())) +\n",
    "        0.1 * (1 - (df[\"Training Time (min)\"] / df[\"Training Time (min)\"].max()))\n",
    "    )\n",
    "    \n",
    "    best_model = df.loc[df[\"Weighted Score\"].idxmax()]\n",
    "    print(\"\\nBest Model Selection:\")\n",
    "    print(f\"Based on weighted criteria (50% F1, 20% speed, 20% size, 10% training time):\")\n",
    "    print(f\"Best model: {best_model['Model']}\")\n",
    "    print(f\"F1-Score: {best_model['F1-Score']:.3f}\")\n",
    "    print(f\"Inference Speed: {best_model['Inference Speed (ms)']:.2f} ms\")\n",
    "    print(f\"Model Size: {best_model['Model Size (MB)']:.2f} MB\")\n",
    "    \n",
    "    return df, best_model\n",
    "\n",
    "# Analyze and visualize results\n",
    "results_df, best_model = analyze_results(comparison_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Main Training Loop\n",
    "**Production-Grade Features**:\n",
    "- Comprehensive resource management\n",
    "- Training progress tracking\n",
    "- Robust evaluation\n",
    "- Detailed result collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_production_recommendation(best_model):\n",
    "    \"\"\"Generate production deployment recommendations\"\"\"\n",
    "    print(\"\\nProduction Deployment Recommendations:\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    print(f\"\\n1. Selected Model: {best_model['Model']}\")\n",
    "    print(f\"   - F1-Score: {best_model['F1-Score']:.3f}\")\n",
    "    print(f\"   - Inference Speed: {best_model['Inference Speed (ms)']:.2f} ms per sample\")\n",
    "    print(f\"   - Model Size: {best_model['Model Size (MB)']:.2f} MB\")\n",
    "    \n",
    "    print(\"\\n2. Deployment Considerations:\")\n",
    "    if best_model[\"Model Size (MB)\"] > 500:\n",
    "        print(\"   - Model is large (>500MB), consider:\")\n",
    "        print(\"     * Using model distillation for production\")\n",
    "        print(\"     * Deploying on GPU-enabled infrastructure\")\n",
    "    else:\n",
    "        print(\"   - Model size is reasonable for production deployment\")\n",
    "    \n",
    "    if best_model[\"Inference Speed (ms)\"] > 50:\n",
    "        print(\"   - Inference speed is moderate, consider:\")\n",
    "        print(\"     * Implementing caching for frequent queries\")\n",
    "        print(\"     * Using batch processing for better throughput\")\n",
    "    else:\n",
    "        print(\"   - Inference speed is excellent for real-time applications\")\n",
    "    \n",
    "    print(\"\\n3. Monitoring Recommendations:\")\n",
    "    print(\"   - Implement performance monitoring for:\")\n",
    "    print(\"     * Model drift (F1-score over time)\")\n",
    "    print(\"     * Inference latency percentiles\")\n",
    "    print(\"     * Error rates by entity type\")\n",
    "    \n",
    "    print(\"\\n4. Future Improvements:\")\n",
    "    print(\"   - Consider ensemble approaches combining top models\")\n",
    "    print(\"   - Implement active learning to improve challenging cases\")\n",
    "    print(\"   - Explore domain-specific pretraining for better accuracy\")\n",
    "\n",
    "# Generate recommendations\n",
    "generate_production_recommendation(best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_production_recommendation(best_model, full_results):\n",
    "    \"\"\"Generate comprehensive production recommendations\"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"PRODUCTION DEPLOYMENT RECOMMENDATIONS\".center(80))\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Model Characteristics\n",
    "    model_info = config.model_candidates.get(best_model['Model'], {})\n",
    "    print(f\"\\n Model Characteristics:\")\n",
    "    print(f\"  - Name: {best_model['Model']}\")\n",
    "    print(f\"  - Type: {model_info.get('description', 'Unknown')}\")\n",
    "    print(f\"  - Expected Performance: {model_info.get('expected_perf', 'Unknown')}\")\n",
    "    print(f\"  - Size: {model_info.get('size', 'Unknown')}\")\n",
    "    \n",
    "    # Deployment Considerations\n",
    "    print(\"\\n Deployment Strategy:\")\n",
    "    if best_model[\"Model Size (MB)\"] > 500:\n",
    "        print(\"  -  Large model detected (>500MB)\")\n",
    "        print(\"     Consider model distillation for production\")\n",
    "        print(\"     GPU acceleration recommended\")\n",
    "        print(\"     Quantization (FP16/INT8) should be evaluated\")\n",
    "    else:\n",
    "        print(\"  -  Model size is production-friendly\")\n",
    "        print(\"     Can be deployed on CPU if needed\")\n",
    "        print(\"     Containerized deployment recommended\")\n",
    "    \n",
    "    # Performance Considerations\n",
    "    print(\"\\n Performance Considerations:\")\n",
    "    if best_model[\"P99 Latency (ms)\"] > 100:\n",
    "        print(\"  -  High tail latency detected\")\n",
    "        print(\"     Implement request batching\")\n",
    "        print(\"     Consider model optimization techniques\")\n",
    "        print(\"     Evaluate hardware acceleration options\")\n",
    "    else:\n",
    "        print(\"  -  Latency is production-suitable\")\n",
    "        print(\"     Real-time processing feasible\")\n",
    "    \n",
    "    # Monitoring Recommendations\n",
    "    print(\"\\n Monitoring Strategy:\")\n",
    "    print(\"  - Essential Metrics:\")\n",
    "    print(\"     Model accuracy drift (weekly)\")\n",
    "    print(\"     P95/P99 latency percentiles\")\n",
    "    print(\"     GPU memory usage (if applicable)\")\n",
    "    print(\"     Per-entity F1 scores\")\n",
    "    \n",
    "    # Improvement Opportunities\n",
    "    print(\"\\n Improvement Opportunities:\")\n",
    "    print(\"  - Potential Next Steps:\")\n",
    "    print(\"     Hyperparameter tuning on larger dataset\")\n",
    "    print(\"     Ensemble with second-best model (F1: {:.3f})\".format(\n",
    "        full_results[full_results['Model'] != best_model['Model']]['F1-Score'].max()))\n",
    "    print(\"     Domain-specific pretraining\")\n",
    "    print(\"     Error analysis session for failure modes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Main Execution\n",
    "**Final Top Practitioner Tips**:\n",
    "- Sequential execution with progress tracking\n",
    "- Intermediate result saving\n",
    "- Comprehensive cleanup\n",
    "- Final reporting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"Complete execution pipeline with robust handling\"\"\"\n",
    "    try:\n",
    "        # Initialize results tracking\n",
    "        comparison_results = []\n",
    "        \n",
    "        # Run comparison for each model\n",
    "        for model_name in config.model_candidates:\n",
    "            print(f\"\\n{'='*50}\")\n",
    "            print(f\"Starting evaluation for: {model_name}\")\n",
    "            print(f\"{'='*50}\")\n",
    "            \n",
    "            result = train_and_evaluate_model(model_name)\n",
    "            comparison_results.append(result)\n",
    "            \n",
    "            # Save intermediate results\n",
    "            with open(config.results_file, 'w') as f:\n",
    "                json.dump(comparison_results, f, indent=2)\n",
    "            \n",
    "            # Print interim summary\n",
    "            if result.get('success', False):\n",
    "                print(f\"\\nCompleted {model_name}:\")\n",
    "                print(f\"  - F1: {result['metrics']['overall_f1']:.3f}\")\n",
    "                print(f\"  - Time: {result['training_time']/60:.1f} min\")\n",
    "                print(f\"  - Size: {result['model_size']:.1f} MB\")\n",
    "            else:\n",
    "                print(f\"\\n Failed {model_name}: {result.get('error', 'Unknown error')}\")\n",
    "        \n",
    "        # Final analysis\n",
    "        results_df, best_model = analyze_results(comparison_results)\n",
    "        generate_production_recommendation(best_model, results_df)\n",
    "        \n",
    "        # Save final report\n",
    "        report = {\n",
    "            \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "            \"best_model\": best_model['Model'],\n",
    "            \"results\": comparison_results,\n",
    "            \"analysis\": results_df.to_dict(orient='records')\n",
    "        }\n",
    "        with open(config.output_dir / \"final_report.json\", 'w') as f:\n",
    "            json.dump(report, f, indent=2)\n",
    "            \n",
    "        return results_df, best_model\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Main execution failed: {str(e)}\", exc_info=True)\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the Complete Analysis\n",
    "**Note**: This will execute the full model comparison pipeline which may take several hours depending on hardware."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
